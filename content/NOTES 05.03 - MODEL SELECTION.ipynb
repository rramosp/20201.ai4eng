{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!wget --no-cache -O init.py -q https://raw.githubusercontent.com/rramosp/20201.ai4eng/master/init.py\n", "import init; init.init(force_download=False); init.get_weblink()"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "import local.lib.timeseries as ts\n", "%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# RULE: we cannot use the same data to BOTH make choices on our precessing pipeline AND report performance"]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": ["dataset = pd.read_csv(\"local/data/cal_housing.data\")\n", "dataset = dataset.sample(len(dataset))\n", "\n", "d = dataset.iloc[:10000].sample(1000)\n", "X = d.values[:,:-1]\n", "y = d[\"medianHouseValue\"].values\n", "print (X.shape, y.shape)"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LinearRegression\n", "from sklearn.svm import SVR\n", "from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.tree import DecisionTreeRegressor\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import median_absolute_error, r2_score, mean_squared_error"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["def rel_mrae(estimator, X, y):\n", "    preds = estimator.predict(X)\n", "    return np.mean(np.abs(preds-y)/y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Let's consider we are choosing between different models with any crossval technique\n", "\n", "We start by using a regular train/test split with resampling"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import cross_validate, ShuffleSplit"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### MODEL 1"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["estimator1 = DecisionTreeRegressor(max_depth=5)\n", "z1 = cross_validate(estimator1, X, y, return_train_score=True, return_estimator=True,\n", "                    scoring=rel_mrae, cv=ShuffleSplit(n_splits=10, test_size=0.1))"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["z1"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["def report_cv_score(z):\n", "    print (\"test score   %.3f (\u00b1%.4f) with %d splits\"%(np.mean(z[\"test_score\"]), np.std(z[\"test_score\"]), len(z[\"test_score\"])))\n", "    print (\"train score  %.3f (\u00b1%.4f) with %d splits\"%(np.mean(z[\"train_score\"]), np.std(z[\"train_score\"]), len(z[\"train_score\"])))\n", "    \n", "report_cv_score(z1)    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### MODEL 2"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestRegressor\n", "estimator2 = DecisionTreeRegressor(max_depth=10)\n", "z2 = cross_validate(estimator2, X, y, return_train_score=True, return_estimator=True,\n", "                    scoring=rel_mrae, cv=ShuffleSplit(n_splits=10, test_size=0.1))  \n", "report_cv_score(z2)    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### MODEL 3"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LinearRegression\n", "estimator3 = LinearRegression()\n", "z3 = cross_validate(estimator3, X, y, return_train_score=True, return_estimator=True,\n", "                    scoring=rel_mrae, cv=ShuffleSplit(n_splits=10, test_size=0.1))\n", "report_cv_score(z3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Some questions:\n", "\n", "- **what would be our model of choice**: probably estimator2. **CORRECT**\n", "- **what is the performance associated with our choice**: the test score reported by estimator2. **INCORRECT**\n", "- we need to **physically** deliver our trained model to the appropriate business area, but we trained several models for statistical stability. **Which model shall we hand over?**\n", "\n", "\n", "we **cannot** use the same data to make a choice AND report a result.\n", "\n", "observe the performance measured with the models we trained before on **new production** data"]}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [], "source": ["ed = dataset[10000:].sample(1000)\n", "eX = ed.values[:,:-1]\n", "ey = ed[\"medianHouseValue\"].values\n", "print (eX.shape, ey.shape)"]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": ["scores = [rel_mrae(estimator, eX, ey) for estimator in z2[\"estimator\"]]\n", "print (\"scores\", scores)\n", "print (\"mean score %.3f (\u00b1%.4f) with %d splits\"%(np.mean(scores), np.std(scores), len(scores)))\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### use train/val/test splits\n", "\n", "- use train/val to train and select model\n", "- use test split ONLY to measure performance of the selected model\n", "\n", "literature and resources use `test`/`val` interchangeably, or sometimes `dev` instead of `val`\n", "\n", "observe how we:\n", "\n", "- do a first split with 10% for test and 90% for train/val\n", "- adjust val size so that when selecting x% of 90% train/val, ends up having the same number of elements as in test"]}, {"cell_type": "code", "execution_count": 59, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "test_size = 0.3\n", "number_of_houses_for_trainval = 1000\n", "\n", "val_size  = test_size/(1-test_size) # so that the have the same number of elements\n", "\n", "assert number_of_houses_for_trainval<10000, \"too many houses for trainval\"\n", "\n", "\n", "d = dataset.iloc[:10000].sample(number_of_houses_for_trainval)\n", "X = d.values[:,:-1]\n", "y = d[\"medianHouseValue\"].values\n", "print (X.shape, y.shape)\n", "\n", "print (\"test size %.2f\"%test_size)\n", "print (\"val size is %.2f (relative to %.2f) \"%(val_size, 1-test_size))\n", "\n", "Xtv, Xts, ytv, yts = train_test_split(X, y, test_size=test_size)\n", "print (Xtv.shape, Xts.shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**PART 1**: we **AUTOMATE** the model selection process"]}, {"cell_type": "code", "execution_count": 60, "metadata": {}, "outputs": [], "source": ["zscores = []\n", "estimators = [estimator1, estimator2, estimator3]\n", "for estimator in estimators:\n", "    print(\"--\")\n", "    z = cross_validate(estimator, Xtv, ytv, return_train_score=True, return_estimator=False,\n", "                       scoring=rel_mrae, cv=ShuffleSplit(n_splits=10, test_size=val_size))\n", "    report_cv_score(z)\n", "    zscores.append(np.mean(z[\"test_score\"]))\n", "best = np.argmin(zscores)\n", "print (\"selecting \", best)\n", "best_estimator = estimators[best]\n", "print (\"\\nselected model\")\n", "print (best_estimator)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**PART 2**: train selected estimator on train/val, report performance on est"]}, {"cell_type": "code", "execution_count": 61, "metadata": {}, "outputs": [], "source": ["best_estimator.fit(Xtv,ytv)\n", "reported_performance = rel_mrae(best_estimator, Xts, yts)\n", "print (\"reported performance of selectd model %.3f\"%reported_performance)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## monitor production\n", "\n", "observe that even when showing **NEW** data to our selected model, performance still varies.\n", "\n", "as we have more data kept in this dataset we **SIMULATE** we get new data batches from production.\n", "\n", "in **SMALL DATA** performance biases are **DIFFICULT** to overcome in practice and must be monitored!!!\n", "\n", "how much data is __small data__ depends on your problem.\n", "\n"]}, {"cell_type": "code", "execution_count": 62, "metadata": {}, "outputs": [], "source": ["er = []\n", "sizes = []\n", "for _ in range(100):\n", "    size = len(Xts)+np.random.randint(len(Xts))-len(Xts)//2\n", "    ed = dataset.iloc[10000:].sample(size)\n", "    eX = ed.values[:,:-1]\n", "    ey = ed[\"medianHouseValue\"].values\n", "    er.append(rel_mrae(best_estimator, eX, ey))\n", "    sizes.append(size)"]}, {"cell_type": "code", "execution_count": 63, "metadata": {}, "outputs": [], "source": ["plt.scatter(sizes, er)\n", "plt.xlabel(\"production dataset size\")\n", "plt.ylabel(\"rel mrae\")\n", "plt.title(\"performance of observed data batches in production\")\n", "plt.axhline(reported_performance, color=\"red\", ls=\"--\", label=\"reported performance\")\n", "plt.axhline(zscores[best], color=\"green\", ls=\"--\", label=\"val performance of selected model\")\n", "plt.axhline(np.mean(er), color=\"black\", ls=\"--\", label=\"mean performance of production data batches\")\n", "plt.axvline(len(Xts), color=\"black\", alpha=.5, ls=\":\", label=\"test size when reporting performance\")\n", "plt.grid(); plt.legend(loc='center left', bbox_to_anchor=(1, 0.5));\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## suggested experiment\n", "\n", "### change `test_size`  and `number_of_houses_for_trainval` when using train/val/test splits above\n", "\n", "larger values might yield better stability of results\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "p37", "language": "python", "name": "p37"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.6"}}, "nbformat": 4, "nbformat_minor": 2}